experiment:
  name: gpu_optimized
  seed: 42

data:
  nih_root: "datasets/NIH Chest X-Rays Master Datasets/archive"
  chexpert_root: "datasets/Standford ML Group X-Rays Chest Master Datasets/cheXphoto-master/cheXphoto-master"
  csv_path: "datasets/NIH Chest X-Rays Master Datasets/archive/Data_Entry_2017.csv"
  image_size: [320, 320]  # High-resolution input to preserve spatial fidelity of small nodules
  num_workers: 3  # Optimal for stability - 4 workers pushed memory to 94-96% (too risky)
  cache_dir: "datasets/cache"  # Pre-processed images cache (set to null to disable)
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  patient_split: true  # Patient-level splitting to prevent data leakage
  use_roi_extraction: true  # Will be disabled automatically if cache_dir is set

augmentations:
  horizontal_flip_prob: 0.5
  rotation_degrees: 10.0  # Limited to ±10° for anatomical plausibility (per thesis)
  random_noise_std: 0.02  # Increased noise for stronger regularization (was 0.01)
  use_gan_synthetic: false
  # Random Resized Crop (scale 0.9-1.0) is applied automatically in dataset

training:
  batch_size: 24  # Reduced to save memory (16GB RAM limit)
  max_epochs: 40  # Upper limit; early stopping will typically stop at best AUC
  early_stopping_patience: 6  # Stop when val_auc does not improve for 6 epochs (avoids collapse)
  precision: "16-mixed"  # Mixed precision for faster training on GPU (saves VRAM)
  accumulate_grad_batches: 4  # Recommended for 320x320 on 4GB VRAM
  optimizer: adamw
  learning_rate: 0.00005  # 5e-5 for fine-tuning; reduces metric wobbles in later epochs
  weight_decay: 0.01  # STABLE: Back to proven baseline value
  scheduler: cosine
  warmup_epochs: 5  # Gradually increase LR to stabilize DenseNet-121 backbone
  gradient_clip_val: 1.0  # CRITICAL: Prevent gradient explosion that causes model collapse
  use_weighted_sampling: false  # ASL handles imbalance, sampling not needed
  prediction_threshold: 0.15  # Lowered to increase Recall/Sensitivity - flags subtle pathologies for radiological review

model:
  backbone: densenet121
  pretrained: true
  dropout: 0.3  # INCREASED: Higher dropout to prevent overfitting after ASL fix (was 0.25)
  num_classes: 2
  class_weights: [1.0, 4.0]  # Balanced emphasis on Fibrosis (2–4x range from Gemini) to avoid over-correction

loss:
  name: asymmetric  # Asymmetric Loss (ASL) for extreme class imbalance
  gamma_neg: 2.0  # Reduced to "thaw" gradients; keeps loss from collapsing so model keeps learning
  gamma_pos: 1.0  # Keep at 1.0 to let rare positive confidences grow
  clip: 0.01  # Lower clip allows more gradient signal for rare diseases to cross threshold
  bce_aux_weight: 0.1  # Auxiliary BCE keeps gradients alive when ASL collapses; improves AUC/F1
