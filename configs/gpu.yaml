experiment:
  name: gpu_optimized
  seed: 42

data:
  nih_root: "datasets/NIH Chest X-Rays Master Datasets/archive"
  chexpert_root: "datasets/Standford ML Group X-Rays Chest Master Datasets/cheXphoto-master/cheXphoto-master"
  csv_path: "datasets/NIH Chest X-Rays Master Datasets/archive/Data_Entry_2017.csv"
  image_size: [320, 320]  # High-resolution input to preserve spatial fidelity of small nodules
  num_workers: 3  # Optimal for stability - 4 workers pushed memory to 94-96% (too risky)
  cache_dir: "datasets/cache"  # Pre-processed images cache (set to null to disable)
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  patient_split: true  # Patient-level splitting to prevent data leakage
  use_roi_extraction: true  # Will be disabled automatically if cache_dir is set

augmentations:
  horizontal_flip_prob: 0.5
  rotation_degrees: 10.0  # Limited to ±10° for anatomical plausibility (per thesis)
  random_noise_std: 0.02  # Increased noise for stronger regularization (was 0.01)
  use_gan_synthetic: false
  # Random Resized Crop with curriculum learning (more aggressive scale)
  resized_crop_scale: [0.7, 1.0]  # More aggressive augmentation for fine-grained learning
  # Brightness/Contrast jitter for texture recognition
  brightness_jitter: 0.1  # Small brightness variation
  contrast_jitter: 0.1  # Small contrast variation

training:
  batch_size: 24  # Reduced to save memory (16GB RAM limit)
  max_epochs: 20  # Reduced from 25: if best metrics are at 8-15, 20 epochs gives buffer without wasting time
  precision: "16-mixed"  # Mixed precision for faster training on GPU (saves VRAM)
  accumulate_grad_batches: 4  # Recommended for 320x320 on 4GB VRAM
  optimizer: adamw
  learning_rate: 0.00001  # Cool down: smaller LR for precise fine-tuning of pathological textures
  weight_decay: 0.01  # STABLE: Back to proven baseline value
  scheduler: cosine
  warmup_epochs: 5  # Gradually increase LR to stabilize DenseNet-121 backbone
  gradient_clip_val: 1.0  # CRITICAL: Prevent gradient explosion that causes model collapse
  use_weighted_sampling: false  # ASL handles imbalance, sampling not needed
  prediction_threshold: 0.15  # Lower threshold for sensitivity (Recall) on rare life-threatening conditions

model:
  backbone: densenet121
  pretrained: true
  dropout: 0.3  # INCREASED: Higher dropout to prevent overfitting after ASL fix (was 0.25)
  num_classes: 2
  class_weights: [1.0, 3.0]  # Boost for Fibrosis (prevalence <1.5%) to exit trivial solution trap

loss:
  name: asymmetric  # Asymmetric Loss (ASL) for extreme class imbalance
  gamma_neg: 4.0  # Focusing parameter for negative samples
  gamma_pos: 2.0  # Increased from 1.0 to force focus on rare positive samples
  clip: 0.02  # Further reduced from 0.05 for more persistent gradient signal on rare classes
