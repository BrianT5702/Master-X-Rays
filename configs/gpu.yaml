experiment:
  name: gpu_optimized
  seed: 42

data:
  nih_root: "datasets/NIH Chest X-Rays Master Datasets/archive"
  chexpert_root: "datasets/Standford ML Group X-Rays Chest Master Datasets/cheXphoto-master/cheXphoto-master"
  csv_path: "datasets/NIH Chest X-Rays Master Datasets/archive/Data_Entry_2017.csv"
  image_size: [224, 224]
  num_workers: 4 
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  patient_split: true

augmentations:
  horizontal_flip_prob: 0.5
  rotation_degrees: 7.5
  random_noise_std: 0.01
  use_gan_synthetic: false

training:
  batch_size: 32  # Larger batch size for GPU (can increase if you have more VRAM)
  max_epochs: 50
  precision: "16-mixed"  # Mixed precision for faster training on GPU
  optimizer: adamw
  learning_rate: 0.00005  # Further reduced: extreme imbalance needs very careful learning
  weight_decay: 0.01
  scheduler: cosine
  warmup_epochs: 5
  use_weighted_sampling: false  # Can enable for better class balance

model:
  backbone: densenet121
  pretrained: true
  dropout: 0.2
  num_classes: 2
  class_weights: null
    # REMOVED: Focal Loss handles imbalance - extreme weights cause gradient explosion

loss:
  name: focal
  alpha: 0.75
  gamma: 2.0
